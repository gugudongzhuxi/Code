import torch
import torch.nn as nn
from torchvision.io import read_image
from torchvision.transforms import Resize, Grayscale, ToPILImage, ToTensor, Compose
from PIL import Image
import numpy as np
import os
import cv2
import matplotlib.pyplot as plt

def save_image(image, filename):
    """ä¿å­˜å›¾åƒ"""
    if isinstance(image, torch.Tensor):
        image = image.numpy().transpose(1, 2, 0)  # CHW -> HWC
        if image.shape[2] == 1:  # å¦‚æœæ˜¯å•é€šé“å›¾åƒï¼Œç§»é™¤æœ€åä¸€ä¸ªç»´åº¦
            image = image[:, :, 0]
    elif isinstance(image, Image.Image):
        image = np.array(image)
    
    # å¦‚æœå›¾åƒæ˜¯æµ®ç‚¹ç±»å‹ï¼Œå°†å…¶ç¼©æ”¾åˆ° [0, 255] å¹¶è½¬æ¢ä¸º uint8
    if image.dtype == np.float32 or image.dtype == np.float64:
        image = (image * 255).clip(0, 255).astype(np.uint8)
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨ - è‡ªåŠ¨é€‰æ‹©å¯ç”¨ç›®å½•
    possible_dirs = [
        "/data_4T/dlg/code/å¯¹è§’å·ç§¯",
        "./output",  # å½“å‰ç›®å½•ä¸‹çš„outputæ–‡ä»¶å¤¹
        ".",  # å½“å‰ç›®å½•
        os.path.expanduser("~/Desktop"),  # æ¡Œé¢
        "/tmp"  # ä¸´æ—¶ç›®å½•
    ]
    
    output_dir = None
    for dir_path in possible_dirs:
        try:
            os.makedirs(dir_path, exist_ok=True)
            # æµ‹è¯•æ˜¯å¦å¯å†™
            test_file = os.path.join(dir_path, "test_write.tmp")
            with open(test_file, 'w') as f:
                f.write("test")
            os.remove(test_file)
            output_dir = dir_path
            break
        except:
            continue
    
    if output_dir is None:
        print("è­¦å‘Šï¼šæ‰¾ä¸åˆ°å¯å†™ç›®å½•ï¼Œå›¾åƒå¯èƒ½æ— æ³•ä¿å­˜")
        return
    
    file_path = os.path.join(output_dir, filename)
    
    # å¦‚æœæ˜¯ç°åº¦å›¾åƒï¼Œéœ€è¦ç‰¹åˆ«å¤„ç†ä»¥æ­£ç¡®ä¿å­˜
    try:
        if len(image.shape) == 2 or (len(image.shape) == 3 and image.shape[2] == 1):
            cv2.imwrite(file_path, image)
        else:
            cv2.imwrite(file_path, cv2.cvtColor(image, cv2.COLOR_RGB2BGR))
        print(f"å·²ä¿å­˜: {file_path}")
    except Exception as e:
        print(f"ä¿å­˜å›¾åƒå¤±è´¥ {filename}: {e}")

# åŠ è½½å•å¼ å›¾åƒ - è‡ªåŠ¨å¤„ç†è·¯å¾„é—®é¢˜
def load_image():
    # å¯èƒ½çš„å›¾åƒè·¯å¾„åˆ—è¡¨
    possible_paths = [
        '/data_4T/dlg/code/å¯¹è§’å·ç§¯/954c02ace1b64150a6d87535155aa342.jpg',
        # './20250529-102704.jpg',  # å½“å‰ç›®å½•
        # '../20250529-102704.jpg',  # ä¸Šçº§ç›®å½•
    ]
    
    # å°è¯•æ‰¾åˆ°å›¾åƒæ–‡ä»¶
    for path in possible_paths:
        if os.path.exists(path):
            print(f"æ‰¾åˆ°å›¾åƒæ–‡ä»¶: {path}")
            return read_image(path)
    
    # å¦‚æœæ‰¾ä¸åˆ°æ–‡ä»¶ï¼Œåˆ›å»ºä¸€ä¸ªæµ‹è¯•å›¾åƒ - æ¨¡æ‹Ÿå»ºç­‘å·¥åœ°åœºæ™¯
    print("æœªæ‰¾åˆ°æŒ‡å®šå›¾åƒæ–‡ä»¶ï¼Œåˆ›å»ºå»ºç­‘å·¥åœ°æµ‹è¯•å›¾åƒ...")
    # åˆ›å»ºä¸€ä¸ª256x256çš„æµ‹è¯•å›¾åƒï¼ŒåŒ…å«å»ºç­‘å·¥åœ°å…ƒç´ 
    test_img = torch.zeros(3, 256, 256)
    
    # æ·»åŠ å»ºç­‘å·¥åœ°ç‰¹å¾
    for i in range(256):
        for j in range(256):
            # åŠè½¦è‡‚ - æ°´å¹³æ–¹å‘
            if 80 <= i <= 85 and 50 <= j <= 200:  # æ°´å¹³åŠè½¦è‡‚
                test_img[:, i, j] = 0.9
            if 60 <= i <= 65 and 150 <= j <= 220:  # å¦ä¸€ä¸ªæ°´å¹³è‡‚
                test_img[:, i, j] = 0.8
                
            # å¡”åŠ - å‚ç›´æ–¹å‘  
            elif 50 <= i <= 180 and 120 <= j <= 125:  # å‚ç›´å¡”èº«
                test_img[:, i, j] = 0.7
            elif 30 <= i <= 200 and 75 <= j <= 80:   # å¦ä¸€ä¸ªå‚ç›´ç»“æ„
                test_img[:, i, j] = 0.6
                
            # æŒ–æ˜æœºè‡‚ - å¯¹è§’çº¿æ–¹å‘
            elif abs(i - j + 50) < 3 and 100 <= i <= 180 and 50 <= j <= 130:  # æ–œè‡‚1
                test_img[:, i, j] = 1.0
            elif abs(i + j - 300) < 3 and 120 <= i <= 180 and 120 <= j <= 180:  # æ–œè‡‚2
                test_img[:, i, j] = 0.9
                
            # æ··å‡åœŸæ³µç®¡ - æ°´å¹³å»¶å±•
            elif 150 <= i <= 155 and 30 <= j <= 180:
                test_img[:, i, j] = 0.5
                
            # é’¢ç­‹ - å‚ç›´æ’åˆ—
            elif j % 20 == 0 and 200 <= i <= 240:
                test_img[:, i, j] = 0.4
                
            # æ·»åŠ ä¸€äº›èƒŒæ™¯å™ªå£°
            else:
                test_img[:, i, j] = 0.1 + 0.1 * torch.rand(1)
    
    return (test_img * 255).byte()

# å®šä¹‰å›¾åƒå¤„ç†æµç¨‹
transform = Compose([
    ToPILImage(),
    Resize((256, 256)),
    Grayscale(),
    ToTensor()
])

# åŠ è½½å›¾åƒ
image = load_image()
image = image.float() / 255.0  # å½’ä¸€åŒ–åˆ° [0,1]
image = transform(image)  # åº”ç”¨å˜æ¢
image = image.unsqueeze(0)  # æ·»åŠ  batch ç»´åº¦ -> shape: [1, C, H, W]
print("å›¾åƒå½¢çŠ¶ï¼š", image.shape)

class DirectionalConvModel(nn.Module):
    """
    ä¸‰æ–¹å‘å·ç§¯æ¨¡å‹ï¼šé€‚ç”¨äºå»ºç­‘å·¥åœ°è®¾å¤‡æ£€æµ‹
    """
    def __init__(self, dim=1):
        super(DirectionalConvModel, self).__init__()
        
        print("åˆå§‹åŒ–ä¸‰æ–¹å‘å·ç§¯æ¨¡å‹...")
        
        # æ°´å¹³æ–¹å‘ï¼šé€‚åˆåŠè½¦è‡‚ã€æ··å‡åœŸæ³µç®¡ç­‰æ¨ªå‘å»¶å±•è®¾å¤‡
        self.conv_horizontal = nn.Conv2d(
            dim, dim, 
            kernel_size=(1, 7), 
            padding=(0, 3), 
            padding_mode='reflect'
        )
        
        # å‚ç›´æ–¹å‘ï¼šé€‚åˆå¡”åŠç­‰çºµå‘å»¶å±•è®¾å¤‡
        self.conv_vertical = nn.Conv2d(
            dim, dim, 
            kernel_size=(7, 1), 
            padding=(3, 0), 
            padding_mode='reflect'
        )
        
        # å¯¹è§’çº¿æ–¹å‘ï¼šé€‚åˆæ–œç½®çš„æŒ–æ˜æœºè‡‚ç­‰
        self.conv_diagonal = nn.Conv2d(
            dim, dim, 
            kernel_size=5, 
            padding=4, 
            dilation=2, 
            padding_mode='reflect'
        )
        
        # åˆå§‹åŒ–æƒé‡ä¸ºæ›´æœ‰æ„ä¹‰çš„æ¨¡å¼
        self._init_directional_weights()
    
    def _init_directional_weights(self):
        """åˆå§‹åŒ–æ–¹å‘æ€§æƒé‡"""
        with torch.no_grad():
            # æ°´å¹³å·ç§¯æ ¸ï¼šå¼ºè°ƒæ°´å¹³è¾¹ç¼˜
            h_kernel = torch.tensor([[[[-1, -1, 0, 1, 1, 1, 0]]]], dtype=torch.float32)
            self.conv_horizontal.weight.data = h_kernel
            self.conv_horizontal.bias.data.fill_(0)
            
            # å‚ç›´å·ç§¯æ ¸ï¼šå¼ºè°ƒå‚ç›´è¾¹ç¼˜
            v_kernel = torch.tensor([[[[-1], [-1], [0], [1], [1], [1], [0]]]], dtype=torch.float32)
            self.conv_vertical.weight.data = v_kernel
            self.conv_vertical.bias.data.fill_(0)
            
            # å¯¹è§’å·ç§¯æ ¸ï¼šå¼ºè°ƒå¯¹è§’è¾¹ç¼˜ (ä½¿ç”¨è†¨èƒ€å·ç§¯)
            d_kernel = torch.tensor([[[[1, 0, 0, 0, -1],
                                      [0, 1, 0, -1, 0],
                                      [0, 0, 0, 0, 0],
                                      [0, -1, 0, 1, 0],
                                      [-1, 0, 0, 0, 1]]]], dtype=torch.float32)
            self.conv_diagonal.weight.data = d_kernel
            self.conv_diagonal.bias.data.fill_(0)
    
    def forward(self, x):
        # åˆ†åˆ«æå–ä¸‰ä¸ªæ–¹å‘çš„ç‰¹å¾
        horizontal_features = self.conv_horizontal(x)
        vertical_features = self.conv_vertical(x)
        diagonal_features = self.conv_diagonal(x)
        
        return horizontal_features, vertical_features, diagonal_features
    
    def get_kernel_info(self):
        """è·å–å·ç§¯æ ¸ä¿¡æ¯"""
        print("\n" + "="*60)
        print("ä¸‰æ–¹å‘å·ç§¯æ ¸è¯¦ç»†ä¿¡æ¯:")
        print("="*60)
        
        print("\nğŸ”„ æ°´å¹³æ–¹å‘å·ç§¯æ ¸ (1Ã—7) - æ£€æµ‹æ°´å¹³è®¾å¤‡:")
        print("ç”¨é€”ï¼šåŠè½¦è‡‚ã€æ··å‡åœŸæ³µç®¡ç­‰æ¨ªå‘å»¶å±•è®¾å¤‡")
        h_kernel = self.conv_horizontal.weight.data.squeeze().numpy()
        print("å½¢çŠ¶:", h_kernel.shape)
        print("æƒé‡:", h_kernel)
        
        print("\nğŸ”„ å‚ç›´æ–¹å‘å·ç§¯æ ¸ (7Ã—1) - æ£€æµ‹å‚ç›´è®¾å¤‡:")
        print("ç”¨é€”ï¼šå¡”åŠç­‰çºµå‘å»¶å±•è®¾å¤‡")
        v_kernel = self.conv_vertical.weight.data.squeeze().numpy()
        print("å½¢çŠ¶:", v_kernel.shape)
        print("æƒé‡:", v_kernel.reshape(-1, 1))
        
        print("\nğŸ”„ å¯¹è§’çº¿æ–¹å‘å·ç§¯æ ¸ (5Ã—5, è†¨èƒ€=2) - æ£€æµ‹æ–œå‘è®¾å¤‡:")
        print("ç”¨é€”ï¼šæ–œç½®çš„æŒ–æ˜æœºè‡‚ç­‰")
        d_kernel = self.conv_diagonal.weight.data.squeeze().numpy()
        print("å½¢çŠ¶:", d_kernel.shape)
        print("æƒé‡:")
        print(d_kernel)
        print("æ³¨æ„ï¼šè†¨èƒ€å·ç§¯ï¼Œå®é™…æ„Ÿå—é‡æ›´å¤§")

# åˆ›å»ºæ¨¡å‹å¹¶è¿è¡Œ
print("\nå¼€å§‹ä¸‰æ–¹å‘ç‰¹å¾æå–åˆ†æ...")
model = DirectionalConvModel(dim=1)

# æ˜¾ç¤ºå·ç§¯æ ¸ä¿¡æ¯
model.get_kernel_info()

# æ¨ç†è¿‡ç¨‹
with torch.no_grad():
    print("\n" + "="*60)
    print("å¼€å§‹ç‰¹å¾æå–...")
    print("="*60)
    
    # è·å–ä¸‰ä¸ªæ–¹å‘çš„ç‰¹å¾å›¾
    horizontal_features, vertical_features, diagonal_features = model(image)
    
    print(f"\næ°´å¹³ç‰¹å¾å›¾å½¢çŠ¶: {horizontal_features.shape}")
    print(f"å‚ç›´ç‰¹å¾å›¾å½¢çŠ¶: {vertical_features.shape}")
    print(f"å¯¹è§’ç‰¹å¾å›¾å½¢çŠ¶: {diagonal_features.shape}")
    
    # ä¿å­˜åŸå§‹å›¾åƒ
    original = image[0, 0].cpu().numpy()
    save_image(original, '0_original_image.png')
    
    # ä¿å­˜æ°´å¹³ç‰¹å¾å›¾
    h_feature = horizontal_features[0, 0].cpu().numpy()
    save_image(h_feature, '1_horizontal_features.png')
    print("ğŸ”„ æ°´å¹³ç‰¹å¾å›¾å·²ä¿å­˜ - æ˜¾ç¤ºæ¨ªå‘è®¾å¤‡ï¼ˆåŠè½¦è‡‚ã€æ³µç®¡ç­‰ï¼‰")
    
    # ä¿å­˜å‚ç›´ç‰¹å¾å›¾
    v_feature = vertical_features[0, 0].cpu().numpy()
    save_image(v_feature, '2_vertical_features.png')
    print("ğŸ”„ å‚ç›´ç‰¹å¾å›¾å·²ä¿å­˜ - æ˜¾ç¤ºçºµå‘è®¾å¤‡ï¼ˆå¡”åŠç­‰ï¼‰")
    
    # ä¿å­˜å¯¹è§’ç‰¹å¾å›¾
    d_feature = diagonal_features[0, 0].cpu().numpy()
    save_image(d_feature, '3_diagonal_features.png')
    print("ğŸ”„ å¯¹è§’ç‰¹å¾å›¾å·²ä¿å­˜ - æ˜¾ç¤ºæ–œå‘è®¾å¤‡ï¼ˆæŒ–æ˜æœºè‡‚ç­‰ï¼‰")
    
    # åˆ›å»ºç»„åˆç‰¹å¾å›¾
    combined = horizontal_features + vertical_features + diagonal_features
    combined_feature = combined[0, 0].cpu().numpy()
    save_image(combined_feature, '4_combined_features.png')
    print("ğŸ”„ ç»„åˆç‰¹å¾å›¾å·²ä¿å­˜ - æ‰€æœ‰æ–¹å‘ç‰¹å¾çš„ç»„åˆ")
    
    # ç»Ÿè®¡åˆ†æ
    print("\n" + "="*60)
    print("ç‰¹å¾å›¾ç»Ÿè®¡åˆ†æ:")
    print("="*60)
    
    def analyze_features(features, name):
        feat = features[0, 0].cpu().numpy()
        print(f"\n{name}:")
        print(f"  å‡å€¼: {feat.mean():.4f}")
        print(f"  æ ‡å‡†å·®: {feat.std():.4f}")
        print(f"  æœ€å¤§å€¼: {feat.max():.4f}")
        print(f"  æœ€å°å€¼: {feat.min():.4f}")
        print(f"  æ¿€æ´»åƒç´ æ•° (>0.1): {(feat > 0.1).sum()}")
    
    analyze_features(horizontal_features, "ğŸ”„ æ°´å¹³ç‰¹å¾ (æ£€æµ‹æ¨ªå‘è®¾å¤‡)")
    analyze_features(vertical_features, "ğŸ”„ å‚ç›´ç‰¹å¾ (æ£€æµ‹çºµå‘è®¾å¤‡)")  
    analyze_features(diagonal_features, "ğŸ”„ å¯¹è§’ç‰¹å¾ (æ£€æµ‹æ–œå‘è®¾å¤‡)")

print("\n" + "="*60)
print("ğŸ‰ ä¸‰æ–¹å‘ç‰¹å¾æå–å®Œæˆï¼")
print("="*60)
print("ç”Ÿæˆçš„æ–‡ä»¶è¯´æ˜:")
print("ğŸ“¸ 0_original_image.png - åŸå§‹è¾“å…¥å›¾åƒ")
print("â¡ï¸  1_horizontal_features.png - æ°´å¹³æ–¹å‘ç‰¹å¾ï¼ˆåŠè½¦è‡‚ã€æ³µç®¡ï¼‰")
print("â¬†ï¸  2_vertical_features.png - å‚ç›´æ–¹å‘ç‰¹å¾ï¼ˆå¡”åŠï¼‰")
print("â†—ï¸  3_diagonal_features.png - å¯¹è§’æ–¹å‘ç‰¹å¾ï¼ˆæŒ–æ˜æœºè‡‚ï¼‰")
print("ğŸ”€ 4_combined_features.png - ç»„åˆç‰¹å¾å›¾")
print("\nğŸ’¡ åº”ç”¨åœºæ™¯:")
print("- æ°´å¹³ç‰¹å¾ï¼šæ£€æµ‹åŠè½¦è‡‚ã€æ··å‡åœŸæ³µç®¡ç­‰æ¨ªå‘å»¶å±•è®¾å¤‡")
print("- å‚ç›´ç‰¹å¾ï¼šæ£€æµ‹å¡”åŠã€èµ·é‡æœºç­‰çºµå‘å»¶å±•è®¾å¤‡")
print("- å¯¹è§’ç‰¹å¾ï¼šæ£€æµ‹æŒ–æ˜æœºè‡‚ã€æ–œå¡é“ç­‰å€¾æ–œç»“æ„")
print("\nğŸš€ ä½¿ç”¨æ–¹æ³•ï¼šç›´æ¥è¿è¡Œæ­¤ä»£ç å³å¯ï¼")

# ============================================================================
# ä¸»ç¨‹åºå…¥å£ - å¦‚æœç›´æ¥è¿è¡Œæ­¤è„šæœ¬ä¼šæ‰§è¡Œ
# ============================================================================
if __name__ == "__main__":
    print("ğŸ—ï¸ å»ºç­‘å·¥åœ°è®¾å¤‡æ–¹å‘ç‰¹å¾æå–ç¨‹åºå¯åŠ¨...")
    print("ç¨‹åºå°†è‡ªåŠ¨å±•ç¤ºæ°´å¹³ã€å‚ç›´ã€å¯¹è§’ä¸‰ç§å·ç§¯çš„ç‰¹å¾æå–æ•ˆæœ...")
    # æ‰€æœ‰ä»£ç éƒ½å·²ç»åœ¨ä¸Šé¢æ‰§è¡Œäº†ï¼Œè¿™é‡Œåªæ˜¯ä¸€ä¸ªæ ‡è¯†
    pass